---
layout: paper
id: 40
slides_live_id: 38930718
rocket_id: ool-paper-40
meeting_url: 
authors: "Vincent Sitzmann"
camera_ready: true
cmt_id: -1
kind: oral
session_id: 0
session_title: "Invited Talk"
title: "Implicit Neural Scene Representations"
abstract: "How we represent signals has major implications for the algorithms we build to analyze them. Today, most signals are represented discretely: Images as grids of pixels, shapes as point clouds, audio as grids of amplitudes, etc. If images weren't pixel grids - would we be using convolutional neural networks today? What makes a good or bad representation? Can we do better? I will talk about leveraging emerging implicit neural representations for complex & large signals, such as room-scale geometry, images, audio, video, and physical signals defined via partial differential equations. By embedding an implicit scene representation in a neural rendering framework and learning a prior over these representations, I will show how we can enable 3D reconstruction from only a single posed 2D image. Finally, I will show how gradient-based meta-learning can enable fast inference of implicit representations, and how the features we learn in the process are already useful to the downstream task of semantic segmentation."
track: invited
live: false
video_file_url: none
youtube_url: none
---
